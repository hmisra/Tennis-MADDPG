{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm Used - Multi-Agent DDPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, your agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically,\n",
    "\n",
    "After each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores.\n",
    "This yields a single score for each episode.\n",
    "\n",
    "To solve I trained 2 DDPG agents with a shared actor and critic networks and with shared experience replay. \n",
    "    - critic (value) network that maps (state, action) pairs -> Q-values.\n",
    "    - actor (policy) network that maps states -> actions.\n",
    "\n",
    "DDPG with experience replay uses 2 networks for each actor and critic, one as source network which makes the predictions and other as target which after every set interatctions with the environment updates the source networks. \n",
    "\n",
    "    critic (value) network that maps (state, action) pairs -> Q-values.\n",
    "\n",
    "    actor (policy) network that maps states -> actions.\n",
    "\n",
    "\n",
    "1. Sample Step - In this the we use a actor critic network to smaple possible actions given a set of states. We store the experiences - (state, reward, action, next_state) into a replay buffer. \n",
    "2. Training Step - After pre-decided  in this case 20 timsteps we sample the replay buffer to train the actor and critic networks in the first step.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Whereas MADDPG uses 2 DDPG agents with shared weights between their Critic and Actors and a shared experience replay, but both the DDPG agents do observe different set of Environment states and rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture of the actor and critic networks\n",
    "\n",
    "##### For the actor I used a simple Feed Forward Neural Network with Relu activation and batch normalization layers, below are the layers in order:\n",
    "\n",
    "   Input (size 25 state vector, where 24 env state and +1/-1 for each agent)\n",
    "   \n",
    "        |\n",
    "        \n",
    "     Batch Norm \n",
    "   \n",
    "        |\n",
    "        \n",
    "     Fully Connected\n",
    "   \n",
    "        |\n",
    "        \n",
    "      ReLU\n",
    "      \n",
    "        |\n",
    "        \n",
    "     Batch Norm \n",
    "   \n",
    "        |\n",
    "        \n",
    "     Fully Connected\n",
    "   \n",
    "        |\n",
    "        \n",
    "      ReLU\n",
    "      \n",
    "        |\n",
    "        \n",
    "     Batch Norm \n",
    "   \n",
    "        |\n",
    "        \n",
    "     Fully Connected\n",
    "   \n",
    "        |\n",
    "        \n",
    "       Tanh\n",
    "      \n",
    "      \n",
    "      \n",
    "      \n",
    "###### For the critic I also use a Feed Forward NN with Relu Activation and batch normalization layers, below are the layers in the order\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "   Input (size 25 state vector, where 24 env state and +1/-1 for each agent)\n",
    "   \n",
    "        |\n",
    "        \n",
    "   Batch Norm \n",
    "   \n",
    "        |\n",
    "        \n",
    "   Fully Connected\n",
    "   \n",
    "        |\n",
    "        \n",
    "      ReLU    Action\n",
    "      \n",
    "        | _ _ _ | \n",
    "        \n",
    "            |\n",
    "            \n",
    "          Concat\n",
    "          \n",
    "            |\n",
    "            \n",
    "          Fully Connected\n",
    "          \n",
    "            |\n",
    "            \n",
    "          Relu\n",
    "          \n",
    "            |\n",
    "            \n",
    "          Fully Connected (output size 1, expected reward)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "\n",
    "Following hyperparameters were used to train the DQN\n",
    "```\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 128        # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR_ACTOR = 1e-3         # learning rate of the actor\n",
    "LR_CRITIC = 1e-3        # learning rate of the critic\n",
    "WEIGHT_DECAY = 0        # L2 weight decay\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "It was required that the agents collects an average reward of 0.05 over last 100 iterations to consider the environment solved. We reached the desired average reward in about 1000 iterations \n",
    "\n",
    "![alt text](Score.png \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Enhancements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am planning to implement algorithms like PPO, A3C, and D4PG that use multiple (non-interacting, parallel) copies of the same agent to distribute the task of gathering experience. I plan to study the variance in the performance of various algorithms and analyze various use cases where each one can offer benefit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
